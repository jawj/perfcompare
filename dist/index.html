<!DOCTYPE html>
<html>

<head>
  <title>Parsing Postgres JSON? Don't get the bigints wrong.</title>
  <link rel="stylesheet" href="index.css">
</head>

<body>
  <h1>Parsing Postgres JSON? Don't get the bigints wrong.</h1>
  <h2>Introducing <a href="https://github.com/jawj/json-custom-numbers">json-custom-numbers</a>: a conformant, performant JavaScript JSON library</h2>

  <p>George MacKerron</p>

  <br>
  <p>
    JSON is a simple, human-readable data format that can be serialized/deserialized almost anywhere. Love it or hate it, there's a lot of it about.
  </p>
  <p>
    I used JSON in my first production Postgres database. This sat behind the back-end API for <a href="http://www.mappiness.org.uk">a research app</a>. The app could send back JSON containing any data it liked. From that JSON document, the API picked out a few fields it needed, storing and indexing them as ordinary columns. That done, it shoved the whole JSON object into its own column, preserving the data for me to retrieve and analyse later.
  </p>
  <p>
    Back then, in 2010, the JSON data just went into the database as <code>text</code>. But it wasn't long before Postgres got a native JSON type (<a href="https://paquier.xyz/postgresql-2/postgres-9-2-highlight-json-data-type/">with version 9.2</a> in 2012). <a href="https://www.postgresql.org/docs/current/datatype-json.html">Its JSON support</a> has become steadily more powerful since then.
  </p>
  <p>
    You can now use Postgres not just to store and retrieve JSON, but also to build, transform, index and query it. My own TypeScript/Postgres library uses Postgres's JSON functions <a href="https://jawj.github.io/zapatos/#joins-as-nested-json">to create and return handy nested structures out of lateral joins</a>. This all represents a valuable <a href="https://news.ycombinator.com/item?id=28406334">hybrid of relational and 'NoSQL' database capabilities</a>.
  </p>

  <h2>Trouble with numbers</h2>

  <p>
    JSON has its origins in JavaScript, of course. But there's a potential problem when we use JSON to communicate between Postgres and JavaScript.
  </p>
  <p>
    JavaScript has one kind of number: an <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number">IEEE 754 <code>float64</code></a>. Postgres, of course, has many kinds. Some of these, like <code>bigint</code> or <code>numeric</code>, can represent larger and/or more precise numbers than a <code>float64</code>.
  </p>
  <p>
    JavaScript Postgres drivers typically parse these large or precise values into strings. For example:
  </p>

  <pre>await { rows } = pool.query('SELECT (1e16 + 1)::bigint AS big');
// -> [{ big: '10000000000000001' }]</pre>

  <p>
    That leaves you to choose how to deal with them in your code. In this case, you'd probably pass the stringified Postgres <code>bigint</code> value to JavaScript's <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/BigInt"><code>BigInt()</code></a>.
  </p>
  <p>
    Now: what if Postgres were to return that same <code>bigint</code> to JavaScript as a JSON value? Although <a href="https://datatracker.ietf.org/doc/html/rfc8259#section-6">the JSON spec</a> "allows implementations to set limits on the range and precision of numbers accepted", it doesn't set any limits of its own. Effectively, it allows arbitrarily large numbers — much larger than JavaScript can handle — and so Postgres goes right ahead and serializes them.
  </p>
  <p>
    So a JSON number value from Postgres gets parsed with JavaScript's <code>JSON.parse()</code> and, if it's bigger than JavaScript's <code>Number.MAX_SAFE_INTEGER</code> (or more negative than <code>Number.MIN_SAFE_INTEGER</code>), bad things happen.
  </p>

  <pre>await { rows } = pool.query('SELECT to_json((1e16 + 1)::bigint) AS big');
// -> [{ big: 10000000000000000 }]</pre>

  <p>
    Compare the last two results above. That's right: without any warning, the number we got out of the second query is not the same number Postgres sent.
  </p>
  <p>
    Imagine this was the <code>id</code> value of a table row. Well, now it's the <code>id</code> value of a different table row.
  </p>
  <p>
    <i>[Sinister music plays].</i>
  </p>

  <h2>The solution: custom JSON parsing</h2>

  <p>
    A solution to this nastiness is to get hold of a custom JSON parser that can handle big numbers, and to tell your Postgres driver to use it. For both <a href="https://node-postgres.com/">node-postgres</a> and <a href="https://www.npmjs.com/package/@neondatabase/serverless">@neondatabase/serverless</a>, that looks like this:
  </p>

  <pre>import { types } from '@neondatabase/serverless';  // or from 'pg'

function myJSONParse(json) { /* implementation */ }
types.setTypeParser(types.builtins.JSON, myJSONParse);
types.setTypeParser(types.builtins.JSONB, myJSONParse);</pre>

  <p>
    (You might have thought that you could use <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/parse#the_reviver_parameter">the <code>reviver</code> argument to native <code>JSON.parse()</code></a> to avoid implementing a complete JSON parser. Sadly, you can't: by the time the function you supply as the <code>reviver</code> sees a number, it's already been parsed to a JavaScript <code>float64</code>, and the damage has been done).
  </p>
  <p>
    As I see it, there are three key things we're going to want from a custom JSON parser:
  </p>
  <ol>
    <li>
      First, conformance: to avoid any surprises or complications, it should be a perfect drop-in replacement for <code>JSON.parse()</code>. That means the same API and, critically, the same result for every input (aside from the numbers it handles better).
    </li>
    <li>
      Second, performance: it's never going to match the optimised C++ of native <code>JSON.parse()</code>, but it should be the fastest gosh-darn JavaScript implementation we can come up with. In some common contexts (such as an API that mediates between Postgres and a website or app) it may have <b>a lot</b> of data flowing through it, and CPU cycles mean time, electricity and money.
    </li>
    <li>
      And third, flexibility: when it comes across a large number (or indeed any number) in the JSON input, it should give us the chance to deal with it however we want. That could mean using <code>BigInt</code> or some other library.
    </li>
  </ol>
  <p>
    So: we're looking for a conformant, performant JSON parser that can deal flexibly with large numbers.
  </p>
  <p>
    Searching npm turns up two candidate packages: <a href="https://www.npmjs.com/package/json-bigint">json-bigint</a> and <a href="https://www.npmjs.com/package/lossless-json">lossless-json</a>. Are they up to the job?
  </p>

  <h2>Conformance and performance testing</h2>

  <p>
    Behaving the same way as <code>JSON.parse()</code> means our custom JSON parser should throw errors on the same documents, and return the same parsed results for the rest. So we need a set of well-chosen JSON documents, including all the edge cases we can think of, to test against. Happily, the <a href="https://github.com/nst/JSONTestSuite/tree/master/test_parsing">JSON Parsing Test Suite</a> has our back here, with hundreds of test files of valid, invalid, and ambiguous (by the spec) JSON.
  </p>
  <p>
    Assessing performance against <code>JSON.parse()</code> will also call for one or more JSON documents we can test against. Exactly what to use here is a judgment call, but certainly we want to benchmark the parsing of a wide range of JSON values.
  </p>
    Here, I've plumped for: long strings (such as a blog post or a product description); short strings (such as object keys); strings full of backslash escapes (like <code>\u03B1</code> and <code>\n</code>); long numbers (such as high-resolution latitudes and longitudes); short numbers (such as an id or count); and <code>true</code>, <code>false</code> and <code>null</code>. I've combined these values into objects and arrays, so that we also capture speed on the two JSON container types.
  </p>
  <p>
    For a headline comparison, I've then brought all these types together into one large object: <code>{ "longStrings": ..., "shortStrings": ..., ... }</code>.
  </p>
  <p>
    The final piece of the puzzle is: how do we run the performance tests? Performance benchmarking JavaScript seems to have gone way out of fashion in recent years. jsperf.com is long since defunct. <a href="https://github.com/bestiejs/benchmark.js">benchmark.js</a> (which powered it) hasn't had a commit in five years, and consequently doesn't even know about <code>performance.now()</code>.
  </p>
  <p>
    I've therefore put together a simple head-to-head performance function of my own. It evaluates <code>performance.now()</code> timer resolution, estimates how many iterations <i>N</i> of the provided functions are needed to get an accurate reading, and then runs 100 trials of <i>N</i> iterations each. Finally, it plots a simple histogram to compare <i>operations/second</i> in the two cases, and calculates an appropriate statistic (the <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Mann-Whitney U</a>) to establish whether the two distributions are significantly different.
  </p>

  <h3>json-bigint</h3>

  <p>First up: json-bigint. The widgets below tell the full story.</p>

  <div id="conform-json-bigint"></div>
  <div id="compare-json-bigint"></div>

  <p>
    For conformance, the summary is that json-bigint correctly parses all valid documents, except those that are very deeply nested. Very deeply nested structures overflow the call stack of its recursive implementation.
  </p>
  <p>
    json-bigint is then significantly more lax in what else it accepts than <code>JSON.parse()</code>. It permits numbers in various illegal formats (such as <code>.1</code>, <code>1.</code>, <code>01</code>), isn't bothered by unescaped newlines or invalid Unicode escapes in strings, and allows all sorts (character codes 0 – 32) as whitespace.
  </p>
  <p>
    For performance, the headline number is that it's 6 – 11× slower than <code>JSON.parse()</code> on my mixed JSON test document, depending on the browser and wind direction.
  </p>
  <p>
    Regarding flexibility, json-bigint offers various options, but not the one I really want, which is simply to allow me to supply a custom number-parsing function.
  </p>

  <h3>lossless-json</h3>

  <p>Next: lossless-json. How does it compare?</p>

  <div id="conform-lossless-json"></div>
  <div id="compare-lossless-json"></div>

  <p>
    Conformance-wise, lossless-json's big thing is that it throws errors on duplicate object keys. It calls this a feature and, to be fair, it's fully in line with its "lossless" branding. But it's also a definite point of difference from <code>JSON.parse()</code>.
  </p>
  <p>
    Like json-bigint, and for the same reason, lossless-json fails on deeply nested structures. Elsewhere, it's not as lax as json-bigint, but it's still a touch more relaxed than <code>JSON.parse()</code> on number formats, allowing a leading decimal point with no zero (<code>.1</code>).
  </p>
  <p>
    Regarding performance, lossless-json does a bit better than json-bigint, with a headline factor of 4 – 7× slower than <code>JSON.parse</code>.
  </p>
  <p>
    Finally, lossless-json scores points on flexibility by taking a custom number-parsing function as one of its options.
  </p>

  <h2>Can we do better?</h2>
  <p>
    Overall, neither package exactly matches the behaviour of <code>JSON.parse()</code>, and neither seems blisteringly quick. Don't think I'm looking a gift-horse in the mouth here. I'm grateful to the maintainers of both packages for doing the hard work of making useful code and documentation available for free.
  </p>
  <p>
    But we can do better on all three criteria I set out above: conformance, performance, and flexibility:
  </p>
  <ul>
    <li>
      We can, of course, choose to fully match the behaviour of <code>JSON.parse()</code>, and to provide wholly customisable number parsing.
    </li>
    <li>
      Less obviously, we can also improve performance substantially.
    </li>
  </ul>

  <h2>Presenting: json-custom-numbers</h2>
  <p>
    To cut to the chase: <a href="https://www.npmjs.com/package/json-custom-numbers">json-custom-numbers</a> is a conformant, performant, flexible new custom JSON parser (and stringifier too).
  </p>

  <div id="conform-json-custom-numbers"></div>
  <div id="compare-json-custom-numbers"></div>

  <p>
    Today's take-home message is: if you need custom parsing of numbers in JSON, use <a href="https://www.npmjs.com/package/json-custom-numbers">json-custom-numbers</a>. It is (I believe) a perfect match for the behaviour of native <code>JSON.parse()</code>, and it's usually only 1.5 – 3× slower, which is substantially quicker than the alternatives.
  </p>
  <p>Speed varies according to the JavaScript engine and what you're parsing, so there are some <a href="https://github.com/jawj/json-custom-numbers#parse">more detailed comparisons in the project README</a>.</p>
  <p>
    To use json-custom-numbers with Neon's serverless driver (or node-postgres) to parse Postgres <code>bigint</code> values as JavaScript <code>BigInt</code> values, you can do this:
  </p>
  <pre>import { types } from '@neondatabase/serverless';  // or from 'pg'
import { parse } from 'json-custom-numbers';

function parseJSONWithBigInts(str) {
  return parse(str, undefined, function (str) {
    const n = +str;
    const safe = n >= Number.MIN_SAFE_INTEGER && n <= Number.MAX_SAFE_INTEGER;
    return safe || /[.eE]/.test(str) ? n : BigInt(str);
  });
}

types.setTypeParser(types.builtins.JSON, parseJSONWithBigInts);
types.setTypeParser(types.builtins.JSONB, parseJSONWithBigInts);</pre>
  <p>
    This code sample uses <code>BigInt</code> only for integers that don't fit in an ordinary number value. That means a Postgres <code>bigint</code> value can end up as either an ordinary number or a <code>BigInt</code>, depending on its size. For sanity, you'll probably want to ensure that anything that <i>might</i> be a <code>BigInt</code> is treated as one, by subsequently manually converting it: <code>BigInt(bigintValueFromPostgres)</code>.
  </p>
  <p>
    This is a fine place to stop reading. Carry on if you'd like me to point out a few things I learned in the process of writing and optimising the library.
  </p>

  <h2>What I learned</h2>

  <h3>Sticky RegExps rock</h3>

  <p>
    Discovery number one is that <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp/sticky">'sticky' RegExps</a> plus the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp/test">RegExp <code>test()</code> method</a> are a parser-writer's best friend.
  </p>
  <p>
    A sticky RegExp is one created with the <code>y</code> flag. It has a <code>lastIndex</code> property. You can set <code>lastIndex</code> to the string index where you want your RegExp to begin matching. RegExp methods like <code>test()</code> then set <code>lastIndex</code> to the index where matching ended.
  </p>
  <p>
    The json-custom-numbers <code>parse()</code> function parses all primitive values (strings, numbers, <code>true</code>, <code>false</code>, and <code>null</code>) using sticky RegExps. This gives a major performance boost compared to the other implementations, which step through string input character-by-character.
  </p>

  <h3>Experiment</h3>

  <p>
    It'a an obvious point, but there's no substitute for running experiments and seeing what's quicker.
  </p>
  <p>
    For example, <a href="https://en.wikipedia.org/wiki/Inline_expansion">function inlining is a well-known optimization</a> applied by language compilers of all stripes, <a href="https://www.mattzeunert.com/2015/08/21/toggling-v8-function-inlining-with-node.html">including JavaScript engines like V8</a>. You might therefore think that manually inlining would have little performance impact. But some empirical testing showed that inlining functions to read the next character and to skip whitespace — which my original recursive parsing code had inherited from <a href="https://github.com/douglascrockford/JSON-js/blob/03157639c7a7cddd2e9f032537f346f1a87c0f6d/json_parse.js">Crockford's reference implementation</a> — led to overall performance gains of 10 – 20%.
  </p>
  <p>
    As another example, I had an idea that switching from processing single-character strings (extracted with <code>charAt()</code>) to processing integer character codes (extracted with <code>charCodeAt()</code>) might speed things up in some of the places sticky RegExps couldn't help. Experimentation showed this was true, but the scale of the gains is strongly dependent on the JavaScript engine. The change reduced parsing time by about 10% in Safari (JavaScriptCore), 20% in Chrome (V8), and over 50% in Firefox (SpiderMonkey).
  </p>

  <h3>Code <i>remembers</i></h3>

  <p>
    Probably the nastiest and most maddening thing I learned is that JavaScript code has memory! It matters how much your code has been run already. It also matters <i>what input it's seen</i>.
  </p>
  <p>
    JavaScript engines optimise code progressively, as they discover which functions are 'hot' and where the potential optimisation gains might be highest. Optimisation depends heavily on data types and code paths, and code can also be de-optimised if assumptions made by the engine turn out false. I knew this in principle, but I hadn't thought through the implications for benchmarking.
  </p>
  <p>
    This issue reared its head when I was trying to optimise <code>\uXXXX</code> (Unicode) escape parsing code. In Safari, every approach I could think of benchmarked worse than what I'd started with, which was essentially the Crockford reference implementation. I was surprised by this.
  </p>
  <p>
    I eventually resorted to benchmarking Crockford against Crockford — and found that one copy of an identical implementation was significantly slower than the other (<i>p</i> &lt; 0.001). I then realised that my parsing conformance tests involve lots of invalid JSON input, throwing repeated errors in every possible location.</p>
  <p>
    Being exposed to the tests therefore appears to reduce the performance of any particular parsing code. Skipping the prior conformance check (or running it on a different copy of the same code) could turn 20% slower into 10% faster when I then tested performance differences.
  </p>
  <p>
    You can actually see this effect in action using the conformance/performance widget pairs further up this page. For any pair, you'll generally find that the performance figure is substantially better if you <i>haven't</i> tested conformance since page load than if you have.
  </p>
  <p>
    The good news is that if you're using json-custom-numbers to parse JSON that's coming back from Postgres, everything it sees should be valid JSON, and performance will be best-case.
  </p>

  <h3>Writing aids thinking</h3>

  <p>
    I didn't <a href="https://course.ccs.neu.edu/cs5500f14/Notes/Prototyping1/planToThrowOneAway.html"><i>plan</i> to throw one away</a>. But in the end it was writing about the code that led me to do just that.
  </p>
  <p>
    I thought I'd finished the package, I'd already written most of this post, and I was in the middle of claiming that the json-custom-numbers parser perfectly matches native <code>JSON.parse()</code> behaviour. A caveat the occurred to me, and duly wrote a section about how my implementation was recursive, meaning that really-unusually-deeply-nested JSON structures would overflow the call stack.
  </p>
  <p>
    Seeing it written down, and attempting to justify it, this then seemed kind of lame: if you can see the problem, why not fix it? So I went back and rewrote the parser as <a href="https://github.com/jawj/json-custom-numbers/blob/main/test/test_comparison/parseStateMachine.ts">a nice big state machine</a> instead. Since this was slightly slower than the recursive implementation had been, I then wrote a slightly less elegant but slightly faster <a href="https://github.com/jawj/json-custom-numbers/blob/main/src/parse.ts">final non-recursive implementation</a>.
  </p>

  <h3>You can't natively <code>stringify</code> everything you can <code>parse</code></h3>

  <p>
    The major JavaScript engines all now have non-recursive <code>JSON.parse()</code> implementations. For example, <a href="https://v8.dev/blog/v8-release-76">V8 became non-recursive in 2019</a>.
  </p>
  <p>
    So I was surprised to discover (after writing non-recursive implementations for both <code>parse</code> and <code>stringify</code>) that the native <code>JSON.stringify()</code> implementations still appear to be recursive. Given a deeply-enough nested structure, <code>JSON.stringify()</code> will give you <code>RangeError: Maximum call stack size exceeded</code> or <code>InternalError: too much recursion</code>.
  </p>
  <p>
    This means there are values of <code>n</code> for which <code>let deepObj = JSON.parse('['.repeat(n) + ']'.repeat(n))</code> succeeds, but <code>let deepJSON = JSON.stringify(deepObj)</code> then fails. The smallest value of <code>n</code> where this happens indicates your JavaScript engine's call stack size (today, on my laptop, that smallest <code>n</code> seems to be 3,375 for Firefox, 3,920 for Chrome, 4,639 for Node, and 40,001 for Safari or Bun).
  </p>
  <p>
    You might argue that this is a feature, in that it prevents a circular reference leading to an infinite loop. (Circular references are usually detected, but certain <code>replacer</code> functions can thwart this: for example, <code>let obj = {}; obj.obj = obj; JSON.stringify(obj)</code> gets you a complaint about the circular reference, but <code>JSON.stringify(obj, (k, v) => [v])</code> on the same object overflows the call stack instead).
  </p>
  <p>
    Anyway, for json-custom-numbers I decided to keep my non-recursive <code>stringify</code> implementation, but also to provide a <code>maxDepth</code> option for both <code>stringify</code> and <code>parse</code>. For <code>stringify</code>, <code>maxDepth</code> defaults to 50,000 — a bit higher than you get in Safari and Bun — but can be set to anything up to <code>Infinity</code>. For <code>parse</code>, it defaults to <code>Infinity</code>, which matches the native implementations and means you can go as deep as available memory allows.
  </p>
  <script src="index.js"></script>
</body>

</html>